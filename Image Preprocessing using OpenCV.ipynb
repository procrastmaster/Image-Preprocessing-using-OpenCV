{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision using OpenCV\n",
    "* BGR channels: ranges from 0 to 255\n",
    "* Grayscale: 1 channel, ranges from 0 to 255, 0 = Black, 255 = White\n",
    "* !pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, Show, Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img_path = 'pic1.jpg'\n",
    "\n",
    "# Load image\n",
    "img = cv2.imread(img_path,1) # 1 --> Color, 0 --> Grayscale\n",
    "img_shape = img.shape\n",
    "\n",
    "# Show image\n",
    "cv2.imshow('image',img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Save image\n",
    "# cv2.imwrite('saved image', img)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grayscale, Resize, Crop, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Grayscale\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('image',gray_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Resize image\n",
    "resize_shape = (224,224)\n",
    "\n",
    "resize_img_1 = cv2.resize(img, None,fx=0.5, fy=0.5)\n",
    "cv2.imshow('image',resize_img_1)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "resize_img_2 = cv2.resize(img, resize_shape)\n",
    "cv2.imshow('image',resize_img_2)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Crop image with indexing\n",
    "cropped_img = img[10:100,10:100]\n",
    "cv2.imshow('image',cropped_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw a Line, Circle, Rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Black image\n",
    "black_img = np.zeros((224,224,3))\n",
    "cv2.imshow('image',black_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Create rectangle\n",
    "rect = cv2.rectangle(black_img, (1,1),(100,100),(255,0,0),5)\n",
    "cv2.imshow('image',rect)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Create line\n",
    "line = cv2.line(black_img, (1,1),(100,100),(255,0,0),5)\n",
    "cv2.imshow('image',line)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Create circle\n",
    "circle = cv2.circle(black_img, (100,100), 10, (255,0,0),5)\n",
    "cv2.imshow('image',circle)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotate, Flip, Change perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotate image\n",
    "rotate_img_matrix = cv2.getRotationMatrix2D((img.shape[0], img.shape[1]) , 45,0.5)\n",
    "rotate_img = cv2.warpAffine(img, rotate_img_matrix, (img.shape[0], img.shape[1]))\n",
    "cv2.imshow('image',rotate_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Flip image\n",
    "flip_img = cv2.flip(img,1)\n",
    "cv2.imshow('image',flip_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Change perspective\n",
    "height = 100\n",
    "width = 100\n",
    "\n",
    "orig_img_coor = np.float32([[100,100], [200,100], [100,200], [200,200]])\n",
    "new_img_coor = np.float32([[0,0], [width,0], [0,height], [width,height]])\n",
    "perspective_img_matrix = cv2.getPerspectiveTransform(orig_img_coor, new_img_coor)\n",
    "perspective_img = cv2.warpPerspective(img, perspective_img_matrix, (width,height) )\n",
    "cv2.imshow('image',perspective_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brightness and Darkness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "intensity = 50\n",
    "intensity_matrix = np.ones(img.shape, dtype='uint8') * intensity\n",
    "\n",
    "bright_img = cv2.add(img,intensity_matrix)\n",
    "cv2.imshow('image',bright_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "dark_img = cv2.subtract(img,intensity_matrix)\n",
    "cv2.imshow('image',dark_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blur, Sharpening, Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size_5X5 = np.ones((5,5)) / 25\n",
    "kernel_size_5X5_tuple = (5,5)\n",
    "\n",
    "# Gaussian blur\n",
    "blur_gaussian = cv2.GaussianBlur(img, kernel_size_5X5_tuple, 1)\n",
    "cv2.imshow('image',blur_gaussian)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# filter2D blur\n",
    "blur_filter2D = cv2.filter2D(img, -1, kernel_size_5X5)\n",
    "cv2.imshow('image',blur_filter2D)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Sharpening kernel\n",
    "sharp_filter = np.array([[-1,-1,-1],[-1,10,-1],[-1,-1,-1]])\n",
    "# Sharp image\n",
    "sharp_img = cv2.filter2D(img,-1, sharp_filter)\n",
    "cv2.imshow('image', sharp_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Check out cv2.THRESH_BINARY_INV, cv2.THRESH_TRUNC \n",
    "# Values below 100 set to 0 (black), Values above 100 set to 255 (white)\n",
    "ret, threshold_img = cv2.threshold(gray_img, 100, 255, cv2.THRESH_BINARY)\n",
    "cv2.imshow('image', threshold_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morphological Transformations\n",
    "* Erosion: Background erodes\n",
    "* Dilation: Background dilates\n",
    "* Opening = Erosion >>> Dilation\n",
    "* Closing = Dilation >>> Erosion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size_5X5 = np.ones((5,5)) / 25\n",
    "\n",
    "# Erosion\n",
    "erosion_img = cv2.erode(img, kernel_size_5X5)\n",
    "cv2.imshow('image', erosion_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Dilation\n",
    "dilation_img = cv2.dilate(img,kernel_size_5X5)\n",
    "cv2.imshow('image', dilation_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Opening operation for noise removal\n",
    "opening_operation = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel_size_5X5)\n",
    "cv2.imshow('image', opening_operation)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "\n",
    "# Closing operation for noise removal\n",
    "closing_operation = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel_size_5X5)\n",
    "cv2.imshow('image', closing_operation)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge Detection\n",
    "* Lapacian, Canny, Sobel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canny Edge Detection\n",
    "canny_edge_img = cv2.Canny(img, 10,100)\n",
    "cv2.imshow('image', canny_edge_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Laplacian Edge Detection\n",
    "laplacian_edge_img = cv2.Laplacian(img, cv2.CV_64F)\n",
    "cv2.imshow('image', laplacian_edge_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blob Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_detector_object = cv2.SimpleBlobDetector_create()\n",
    "keypoint_info = blob_detector_object.detect(img)\n",
    "blank_img = np.zeros((1,1))\n",
    "blob_detector = cv2.drawKeypoints(img,keypoint_info, np.array([]), \n",
    "                                  (255,0,0), cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "cv2.imshow('image', blob_detector)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line Detection\n",
    "* Probabilistic Hough Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grayscale\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('image',gray_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# detect canny edges\n",
    "canny_edge_img = cv2.Canny(gray_img, 10,100)\n",
    "cv2.imshow('image', canny_edge_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Probabilistic Hough Lines\n",
    "line_info = cv2.HoughLinesP(canny_edge_img, 1, np.pi/180, 250, 110, 10)\n",
    "\n",
    "while line_info is True:\n",
    "    for i in line_info:\n",
    "        for x1,y1,x2,y2 in i:\n",
    "            cv2.line(img, (x1,y1), (x2,y2), (255,0,0), 2)\n",
    "\n",
    "cv2.imshow('image', gray_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contour Detection, Approximating contours and Convex Hull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_copy = img.copy()\n",
    "# grayscale\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('image',gray_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# detect canny edges\n",
    "canny_edge_img = cv2.Canny(gray_img, 10,100)\n",
    "cv2.imshow('image', canny_edge_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Contour detection\n",
    "contours, hierarchy = cv2.findContours(canny_edge_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "contour_img = cv2.drawContours(img_copy, contours,-1,(255,0,0), 4)\n",
    "cv2.imshow('image', contour_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Bounding rectangle around each contour\n",
    "for i in contours:\n",
    "    x,y,w,h = cv2.boundingRect(i)\n",
    "    bounding_rect_img = cv2.rectangle(img_copy, (x,y),(x+w,y+h), (0,0,255),1)\n",
    "    cv2.imshow('image', bounding_rect_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Contour Approximation through each detected contour\n",
    "for i in contours:\n",
    "    accuracy = 0.04 * cv2.arcLength(i, True)\n",
    "    approx = cv2.approxPolyDP(i, accuracy, True)\n",
    "    approx_contour_img = cv2.drawContours(img_copy, [approx],-1,(255,0,0), 4)\n",
    "    cv2.imshow('image', approx_contour_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Convex Hull:\n",
    "\n",
    "# Sort the contours by area and remove the largest contour\n",
    "n = len(contours) - 1\n",
    "contours_sorted = sorted(contours, key=cv2.contourArea, reverse=False)[:n]\n",
    "\n",
    "# Convex Hull through each detected contour\n",
    "for i in contours:\n",
    "    hull = cv2.convexHull(i)\n",
    "    hull_contour_img = cv2.drawContours(img_copy, [hull], -1, (255,0,0), 2)\n",
    "    cv2.imshow('image', hull_contour_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# # Matching contour shapes: compare input image and target image\n",
    "\n",
    "# # contours, hierarchy = cv2.findContours(input_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "# # In input image, First largest contour is the boundary of the entire image\n",
    "# second_largest_contour = contours[1]\n",
    "\n",
    "# # In target image\n",
    "# # contours, hierarchy = cv2.findContours(target_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "\n",
    "# for i in contours:\n",
    "#     match = cv2.matchShapes(second_largest_contour, i, 1, 0)\n",
    "#     if match < 0.2: \n",
    "#         closest_contour = i\n",
    "#     else: \n",
    "#         closest_contour = [0]\n",
    "        \n",
    "# closest_contour_img = cv2.drawContours(img_copy, [closest_contour], -1, (255,0,0), 2)\n",
    "# cv2.imshow('image', closest_contour_img)\n",
    "# cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corner Detection\n",
    "* Harris Corners are detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grayscale\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('image',gray_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "img_copy = img.copy()\n",
    "gray_img_copy = gray_img.copy()\n",
    "\n",
    "harris_corner_info = cv2.cornerHarris(gray_img,3,3,0.05)\n",
    "\n",
    "# Dilate the corner points to enlage them\n",
    "kernel_size_5X5 = np.ones((5,5))/25\n",
    "harris_corner_info = cv2.dilate(harris_corner_info,kernel_size_5X5)\n",
    "\n",
    "img_copy[ harris_corner_info > 0.025 * harris_corner_info.max() ] = [255,0,0]\n",
    "cv2.imshow('image', img_copy)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face and Eye Detection using HAAR Cascade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grayscale\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('image',gray_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "img_copy = img.copy()\n",
    "gray_img_copy = gray_img.copy()\n",
    "\n",
    "# Load cascade classifier\n",
    "classify_face = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "classify_eye = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
    "\n",
    "def detect_face_and_eye(img_copy):\n",
    "    \n",
    "    # Region of interest of detected face\n",
    "    face_roi = classify_face.detectMultiScale(gray_img_copy, 1.3, 5)\n",
    "\n",
    "    if face_roi is False:\n",
    "        print('No face detected')\n",
    "\n",
    "    for fx,fy,fw,fh in face_roi:\n",
    "        face_detected = cv2.rectangle(img_copy, (fx,fy), (fx+fw,fy+fh), (0,0,255), 2)\n",
    "\n",
    "        gray_img_crop = gray_img_copy[fy:fy+fh, fx:fx+fw]\n",
    "        img_crop = img_copy[fy:fy+fh ,fx:fx+fw]\n",
    "\n",
    "        eye_roi = classify_eye.detectMultiScale(gray_img_crop)\n",
    "        for ex,ey,ew,eh in eye_roi:\n",
    "            eye_detected = cv2.rectangle(img_crop,(ex,ey), (ex+ew,ey+eh), (0,0,255), 2)\n",
    "            \n",
    "        img_crop = cv2.flip(img_crop,1)\n",
    "        return img_crop\n",
    "\n",
    "detect_face_and_eye(img_copy)\n",
    "\n",
    "cv2.imshow('image', face_detected)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.imshow('image', eye_detected)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Face and Eye Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    cv2.imshow('video', detect_face_and_eye(frame))\n",
    "    \n",
    "    # Press q to Quit window\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Analysis using OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify_pedestrian = cv2.CascadeClassifier('haarcascade_fullbody.xml')\n",
    "# cam = cv2.VideoCapture('video1.mp4')\n",
    "\n",
    "# # Loop through video\n",
    "# while cam.isOpened():\n",
    "#     ret, frame = cam.read()\n",
    "#     frame = cv2.resize(frame, None, fx=0.5, fy=0.5, interpolation = cv2.INTER_LINEAR)\n",
    "#     gray_img = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#     pedestrian_roi = classify_pedestrian.detectMultiScale(gray_img, 1.3, 5)\n",
    "    \n",
    "#     for x,y,w,h in pedestrian_detected:\n",
    "#         pedestrian_detected = cv2.rectangle(frame, (x,y), (x+w,y+h), (255,0,0), 2)\n",
    "#         cv2.imshow('video', pedestrian_detected)\n",
    "    \n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cam.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image prepocessing using cv2\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img_path = 'pic1.jpg'\n",
    "resize_shape = (224, 224)\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "# Resize\n",
    "resized_img = cv2.resize(img, resize_shape)\n",
    "cv2.imshow('image',resized_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Grayscale\n",
    "gray_img = cv2.cvtColor(resized_img, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('image',gray_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Normalize\n",
    "resized_img = np.array(resized_img)/255\n",
    "\n",
    "# Add an index \n",
    "# img.shape is (1,224,224,3) 1 is the index for better identification of the image\n",
    "resized_img = resized_img[np.newaxis]\n",
    "\n",
    "cv2.imshow('image',np.squeeze(resized_img))\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
